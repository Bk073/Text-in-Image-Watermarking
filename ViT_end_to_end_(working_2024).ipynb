{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/mae/blob/main/models_mae.py\n",
    "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py\n",
    "# https://arxiv.org/abs/2010.11929\n",
    "# https://arxiv.org/abs/2106.10270\n",
    "# https://arxiv.org/abs/2212.08013\n",
    "# https://github.com/lucidrains/vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://127.0.0.1:8083/notebooks/watermarking/notebooks/v-6/Hidden_discriminator_01_18_modified_output_2_28_restarting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e02f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "# !pip install transformers\n",
    "# https://github.com/FrancescoSaverioZuppichini/ViT#position-embedding\n",
    "# !pip install pytorch_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b567f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab, vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import datasets as dv\n",
    "from torchtext import datasets as dt\n",
    "from torchtext.vocab import vocab, GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torchinfo import summary\n",
    "import string\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torchinfo import summary\n",
    "import re\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
    "import random\n",
    "from skimage.util import random_noise\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "import pytorch_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3624f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = DEVICE= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504abf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "487c298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "# Place-holders\n",
    "SRC_LANGUAGE = TGT_LANGUAGE = 'eng'\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f24102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text_):\n",
    "    text = text_.lower()\n",
    "    text = re.sub(r'\\+d', '', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text.strip()\n",
    "    text = text.split(\" \")[:14]\n",
    "#     pdb.set_trace()\n",
    "    # if len(text) != 14:\n",
    "    #         text.extend((14-len(text))*[\"[PAD]\"])\n",
    "    text = ' '.join(i for i in text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def prepare_data(file_path, save_file):\n",
    "    with open(file_path, 'rb') as f_in:\n",
    "        content = f_in.read().decode('utf-8').split('\\n')\n",
    "\n",
    "        preprocessed_content = [preprocess(i) for i in content]\n",
    "        preprocess_content_ = '\\n'.join(i for i in preprocessed_content)\n",
    "#         print(preprocessed_content, preprocess_content_)\n",
    "        with open(save_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(preprocess_content_)\n",
    "            f.close()\n",
    "        f_in.close()\n",
    "\n",
    "prepare_data(train_filepaths[1], 'train.txt')\n",
    "prepare_data(val_filepaths[1], 'val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec8ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(file_path):\n",
    "    with io.open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield token_transform[SRC_LANGUAGE](line.strip())\n",
    "\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "file_path = './train.txt'\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(file_path),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829e7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, inp_file):\n",
    "        print(\"Loading plain data ...\")\n",
    "        self.inp = []\n",
    "\n",
    "        # Read the EN lines\n",
    "        num_inp_lines = 0\n",
    "        with open(inp_file, \"r\") as ef:\n",
    "            for line in ef:\n",
    "                self.inp.append(line)\n",
    "                num_inp_lines += 1\n",
    "\n",
    "    def __getitem__(self, offset):\n",
    "        en = self.inp[offset]\n",
    "        return en\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a12ba9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 transformer, \n",
    "                 positional_encoding,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.transformer = transformer\n",
    "#         self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "#         self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.vocab_size = tgt_vocab_size\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                src_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        memory = self.transformer.encoder(src_emb, src_mask, src_padding_mask)\n",
    "        return memory\n",
    "    \n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "# Encoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 transformer, \n",
    "                 positional_encoding,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = positional_encoding\n",
    "        self.vocab_size = tgt_vocab_size\n",
    "\n",
    "    def forward(self,\n",
    "                trg: Tensor,\n",
    "                memory: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935332c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    if len(token_ids) < 14:\n",
    "            token_ids.extend((14-len(token_ids))*[PAD_IDX])\n",
    "    else:\n",
    "        token_ids = token_ids[:14]\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "#         tgt_batch.append(text_transform[TGT_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "#     tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d778e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_image(path_):\n",
    "    remove_list = []\n",
    "    for i in glob.glob(path_+'*'):\n",
    "        img = np.array(Image.open(os.path.join(path_,i)))\n",
    "        if img.shape[-1] != 3:\n",
    "            remove_list.append(os.path.join(path_,i))\n",
    "    return remove_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddaa6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatermarkDataset(Dataset):\n",
    "  def __init__(self, cover_img_dir, remove_list, transform_c=None, data_size=None):\n",
    "#     self.tokens = []\n",
    "    self.cover_img_dir = cover_img_dir\n",
    "    self.ls = []\n",
    "    self.transform_c = transform_c\n",
    "#     for sentence in watermark_text:\n",
    "#         self.tokens.append(torch.cat([torch.tensor([BOS_IDX]), sentence, torch.tensor([EOS_IDX])], dim=0))\n",
    "    \n",
    "#     self.tokens = pad_sequence(self.tokens, padding_value=1)\n",
    "    \n",
    "#     self.input_ids = torch.tensor(self.tokens, dtype=torch.long)\n",
    "    for i in os.listdir(self.cover_img_dir):\n",
    "            if i.split('.')[-1] == 'JPEG' or 'jpg':\n",
    "                self.ls.append(os.path.join(self.cover_img_dir,i))\n",
    "    for i in glob.glob(self.cover_img_dir+'*/*'):\n",
    "          self.ls.append(i)\n",
    "                \n",
    "    self.remove_list = remove_list\n",
    "    self.ls = list(set(self.ls) - set(self.remove_list))\n",
    "    \n",
    "    random.shuffle(self.ls)\n",
    "    \n",
    "    self.cover_image_list = self.ls[:data_size]\n",
    "#     self.input_ids = self.tokens[:data_size]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.cover_image_list)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "#     input_ids = self.input_ids[:, index]\n",
    "    cover_img_pt = self.cover_image_list[index]\n",
    "\n",
    "    cover_img = Image.open(cover_img_pt)\n",
    "    cover_img = T.ToTensor()(cover_img)\n",
    "\n",
    "    if self.transform_c:\n",
    "        cover_img = self.transform_c(cover_img)\n",
    "        cover_img = T.ToTensor()(cover_img)\n",
    "    return cover_img#, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6216a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/home/bishwa/coco_dataset/train/'\n",
    "val_dir = '/home/bishwa/coco_dataset/val/'\n",
    "# test_dir = '/home/bishwa/coco_dataset/test/'\n",
    "train_remove = remove_image(train_dir)\n",
    "val_remove = remove_image(val_dir)\n",
    "# test_remove = remove_image(test_dir)\n",
    "\n",
    "train_set = WatermarkDataset(train_dir, train_remove,\n",
    "                                        transform_c=T.Compose([T.Resize(size=(224, 224)),T.ToPILImage()]), \n",
    "                                        data_size=9000\n",
    "                                        )\n",
    "\n",
    "val_set = WatermarkDataset(val_dir, val_remove,\n",
    "                                        transform_c=T.Compose([T.Resize(size=(224, 224)), T.ToPILImage()]), \n",
    "                                        data_size=1000\n",
    "                                        )\n",
    "# test_set = WatermarkDataset(test_dir, test_remove,\n",
    "#                                         transform_c=T.Compose([T.Resize(size=(224, 224)), T.ToPILImage()]), \n",
    "#                                         data_size=1000\n",
    "#                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33da38a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b47cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_dataloader_img = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader_img = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dataloader_img = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49f2ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading plain data ...\n",
      "Loading plain data ...\n"
     ]
    }
   ],
   "source": [
    "train_file_path = './train.txt'\n",
    "train_dataset  = TextDataset(train_file_path)\n",
    "train_iter_w = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=False, \\\n",
    "                                                drop_last=True, num_workers=1, collate_fn=collate_fn)\n",
    "val_dataset = TextDataset('./val.txt')\n",
    "valid_iter_w = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, \\\n",
    "                                                drop_last=True, num_workers=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "344385a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f0ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(Decoder(64, 1000, 0.1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3d2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, in_channels: int=3, patch_size: int=16, emb_size: int = 768, img_size: int = 224):\n",
    "    self.patch_size = patch_size\n",
    "    super().__init__()\n",
    "    self.projection = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "#         Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "    )\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "    self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2, emb_size))\n",
    "\n",
    "  def forward(self, x: Tensor) -> Tensor:\n",
    "    b, _, _, _ = x.shape\n",
    "    x = self.projection(x)\n",
    "    hp, wp = x.shape[2], x.shape[3]\n",
    "    x = Rearrange('b e (h) (w) -> b (h w) e')(x)\n",
    "    x += self.positions # this positions increase the size of the original image\n",
    "#     print(x.shape)\n",
    "    return x, hp, wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d051a654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "PatchEmbedding                           [10, 196, 768]            151,296\n",
       "├─Sequential: 1-1                        [10, 768, 14, 14]         --\n",
       "│    └─Conv2d: 2-1                       [10, 768, 14, 14]         1,377,024\n",
       "==========================================================================================\n",
       "Total params: 1,528,320\n",
       "Trainable params: 1,528,320\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.70\n",
       "==========================================================================================\n",
       "Input size (MB): 14.05\n",
       "Forward/backward pass size (MB): 12.04\n",
       "Params size (MB): 5.51\n",
       "Estimated Total Size (MB): 31.60\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(PatchEmbedding(in_channels=7), [(10, 7, 224, 224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7425613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150528\n",
      "150528\n"
     ]
    }
   ],
   "source": [
    "print(3*224*224)\n",
    "print(196*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14121610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "# patches_embedded = PatchEmbedding()(x)\n",
    "# MultiHeadAttention()(patches_embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1ab7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "  def __init__(self, fn):\n",
    "    super().__init__()\n",
    "    self.fn = fn\n",
    "\n",
    "  def forward(self, x, **kwargs):\n",
    "    res = x\n",
    "    x = self.fn(x, **kwargs)\n",
    "    x += res\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab718a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "  def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "    super().__init__(\n",
    "        nn.Linear(emb_size, expansion * emb_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(drop_p),\n",
    "        nn.Linear(expansion * emb_size, emb_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "008ed5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Sequential):\n",
    "  def __init__(self, emb_size: int=768, n_classes: int = 1000):\n",
    "    super().__init__(\n",
    "        Reduce('b n e -> b e', reduction='mean'),\n",
    "        nn.LayerNorm(emb_size),\n",
    "        nn.Linear(emb_size, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8db72054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "  def __init__(self,\n",
    "               emb_size: int = 768,\n",
    "               drop_p: float = 0.,\n",
    "               forward_expansion: int = 4,\n",
    "               forward_drop_p: float = 0.,\n",
    "               **kwargs):\n",
    "    super().__init__(\n",
    "        ResidualAdd(nn.Sequential(\n",
    "            nn.LayerNorm(emb_size),\n",
    "            MultiHeadAttention(emb_size, **kwargs),\n",
    "            nn.Dropout(drop_p)\n",
    "        )),\n",
    "        ResidualAdd(nn.Sequential(\n",
    "            nn.LayerNorm(emb_size),\n",
    "            FeedForwardBlock(\n",
    "                emb_size, expansion=forward_expansion, drop_p=forward_drop_p\n",
    "            ),\n",
    "            nn.Dropout(drop_p)\n",
    "        ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "557c7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Sequential):\n",
    "  def __init__(self, depth: int=12, **kwargs):\n",
    "    super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8e07306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "               in_channels: int = 3,\n",
    "               patch_size: int = 16,\n",
    "               emb_size: int = 768,\n",
    "               img_size: int = 224,\n",
    "               depth: int = 3,\n",
    "               **kwargs):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
    "        self.transformer_encoder = TransformerEncoder(depth, emb_size=emb_size, **kwargs)\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        b, c, h, w = x.shape[0], x.shape[1], x.shape[2], x.shape[3] \n",
    "        x, hp, wp = self.patch_embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a97aa79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ViT                                                [10, 196, 768]            --\n",
       "├─PatchEmbedding: 1-1                              [10, 196, 768]            151,296\n",
       "│    └─Sequential: 2-1                             [10, 768, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-1                            [10, 768, 14, 14]         1,377,024\n",
       "├─TransformerEncoder: 1-2                          [10, 196, 768]            --\n",
       "│    └─TransformerEncoderBlock: 2-2                [10, 196, 768]            --\n",
       "│    │    └─ResidualAdd: 3-2                       [10, 196, 768]            2,363,904\n",
       "│    │    └─ResidualAdd: 3-3                       [10, 196, 768]            4,723,968\n",
       "│    └─TransformerEncoderBlock: 2-3                [10, 196, 768]            --\n",
       "│    │    └─ResidualAdd: 3-4                       [10, 196, 768]            2,363,904\n",
       "│    │    └─ResidualAdd: 3-5                       [10, 196, 768]            4,723,968\n",
       "│    └─TransformerEncoderBlock: 2-4                [10, 196, 768]            --\n",
       "│    │    └─ResidualAdd: 3-6                       [10, 196, 768]            2,363,904\n",
       "│    │    └─ResidualAdd: 3-7                       [10, 196, 768]            4,723,968\n",
       "====================================================================================================\n",
       "Total params: 22,791,936\n",
       "Trainable params: 22,791,936\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 2.91\n",
       "====================================================================================================\n",
       "Input size (MB): 14.05\n",
       "Forward/backward pass size (MB): 409.44\n",
       "Params size (MB): 90.56\n",
       "Estimated Total Size (MB): 514.05\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary(ViT(), [(2, 3, 224, 224)], device='cpu')\n",
    "summary(ViT(in_channels=7), [(10, 7, 224, 224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00e5da87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151296\n",
      "150528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The other thing on the embedder can be to use the ViT itself to learn the features of watermark\\ninstead of Convolutional layers'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(197*768)\n",
    "print(3*224*224)\n",
    "'''Do we need the class token? How important is it in our task?'''\n",
    "'''Class token is used to accumulate global context which helps in classification task.'''\n",
    "\n",
    "'''How can we validate the ViT representations? Shall we use some pre-trained weights?'''\n",
    "\n",
    "'''Pass an image and we need image as output itself, how does the scaling using convolution work?\n",
    "    Do we need to increase and then decrease the feature space?\n",
    "'''\n",
    "\n",
    "'''The other thing on the embedder can be to use the ViT itself to learn the features of watermark\n",
    "instead of Convolutional layers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0fffafb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Embedder                                                [1, 3, 224, 224]          --\n",
       "├─ViT: 1-1                                              [1, 196, 768]             --\n",
       "│    └─PatchEmbedding: 2-1                              [1, 196, 768]             151,296\n",
       "│    │    └─Sequential: 3-1                             [1, 768, 14, 14]          590,592\n",
       "│    └─TransformerEncoder: 2-2                          [1, 196, 768]             --\n",
       "│    │    └─TransformerEncoderBlock: 3-2                [1, 196, 768]             7,087,872\n",
       "│    │    └─TransformerEncoderBlock: 3-3                [1, 196, 768]             7,087,872\n",
       "│    │    └─TransformerEncoderBlock: 3-4                [1, 196, 768]             7,087,872\n",
       "├─ViT: 1-2                                              [1, 196, 768]             --\n",
       "│    └─PatchEmbedding: 2-3                              [1, 196, 768]             151,296\n",
       "│    │    └─Sequential: 3-5                             [1, 768, 14, 14]          1,377,024\n",
       "│    └─TransformerEncoder: 2-4                          [1, 196, 768]             --\n",
       "│    │    └─TransformerEncoderBlock: 3-6                [1, 196, 768]             7,087,872\n",
       "│    │    └─TransformerEncoderBlock: 3-7                [1, 196, 768]             7,087,872\n",
       "│    │    └─TransformerEncoderBlock: 3-8                [1, 196, 768]             7,087,872\n",
       "=========================================================================================================\n",
       "Total params: 44,797,440\n",
       "Trainable params: 44,797,440\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 428.18\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.61\n",
       "Forward/backward pass size (MB): 81.89\n",
       "Params size (MB): 177.98\n",
       "Estimated Total Size (MB): 260.47\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vit_pre = ViT()\n",
    "        self.vit_embedder = ViT(in_channels=7)\n",
    "    \n",
    "    def forward(self, cover_img, watermark):\n",
    "        # cover image\n",
    "        batch, c, h, w = cover_img.shape[0], cover_img.shape[1], cover_img.shape[2], cover_img.shape[3]\n",
    "        \n",
    "        img_features = self.vit_pre(cover_img)\n",
    "        img_features = torch.reshape(img_features, (batch, c, h, w))\n",
    "        \n",
    "        # text\n",
    "#         print(watermark.shape)\n",
    "        w_m = watermark.permute(1, 0, 2)\n",
    "#         print(w_m.shape)\n",
    "        w_m = w_m.repeat(1, 49, 1)\n",
    "        w_m = torch.reshape(w_m, (batch, -1, 224, 224))\n",
    "        \n",
    "        watermark = torch.cat([w_m, img_features, cover_img], dim=1) # 64+1+3\n",
    "#         pdb.set_trace()\n",
    "        watermark = self.vit_embedder(watermark)\n",
    "        watermark = torch.reshape(watermark, (batch, c, h, w))\n",
    "#         watermark = watermark + img_features\n",
    "        watermark = 0.8*watermark + cover_img # 0.8\n",
    "        \n",
    "        return watermark\n",
    "\n",
    "summary(Embedder(), [(1, 3, 224, 224), (16, 1, 64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76191447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vit = ViT()\n",
    "        self.outer = ClassificationHead(768, 1024)\n",
    "\n",
    "    def forward(self, watermark_img): \n",
    "        features = self.vit(watermark_img)\n",
    "        out = self.outer(features)\n",
    "        out = torch.reshape(out, (out.size()[0], 16, 64)) # 16, batch, 64\n",
    "#         watermark = torch.reshape(watermark, (watermark.size()[0], 8, 64)) # 16, batch, 64\n",
    "        \n",
    "        return out.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7952bfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Extractor                                               [16, 1, 64]               --\n",
       "├─ViT: 1-1                                              [1, 196, 768]             --\n",
       "│    └─PatchEmbedding: 2-1                              [1, 196, 768]             151,296\n",
       "│    │    └─Sequential: 3-1                             [1, 768, 14, 14]          590,592\n",
       "│    └─TransformerEncoder: 2-2                          [1, 196, 768]             --\n",
       "│    │    └─TransformerEncoderBlock: 3-2                [1, 196, 768]             7,087,872\n",
       "│    │    └─TransformerEncoderBlock: 3-3                [1, 196, 768]             7,087,872\n",
       "│    │    └─TransformerEncoderBlock: 3-4                [1, 196, 768]             7,087,872\n",
       "├─ClassificationHead: 1-2                               [1, 1024]                 --\n",
       "│    └─Reduce: 2-3                                      [1, 768]                  --\n",
       "│    └─LayerNorm: 2-4                                   [1, 768]                  1,536\n",
       "│    └─Linear: 2-5                                      [1, 1024]                 787,456\n",
       "=========================================================================================================\n",
       "Total params: 22,794,496\n",
       "Trainable params: 22,794,496\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 137.81\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 40.96\n",
       "Params size (MB): 90.57\n",
       "Estimated Total Size (MB): 132.13\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Extractor(), [(1,3,224, 224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06d56371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNRelu(nn.Module):\n",
    "    \"\"\"\n",
    "    Building block used in HiDDeN network. Is a sequence of Convolution, Batch Normalization, and ReLU activation\n",
    "    \"\"\"\n",
    "    def __init__(self, channels_in, channels_out, stride=1):\n",
    "\n",
    "        super(ConvBNRelu, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(channels_in, channels_out, 3, stride, padding=1),\n",
    "            nn.BatchNorm2d(channels_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator network. Receives an image and has to figure out whether it has a watermark inserted into it, or not.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # 3 blocks, 64 channels\n",
    "        layers = [ConvBNRelu(3, 64)]\n",
    "        for _ in range(3):\n",
    "            layers.append(ConvBNRelu(64, 64))\n",
    "\n",
    "        layers.append(nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
    "        self.before_linear = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, image):\n",
    "        X = self.before_linear(image)\n",
    "        # the output is of shape b x c x 1 x 1, and we want to squeeze out the last two dummy dimensions and make\n",
    "        # the tensor of shape b x c. If we just call squeeze_() it will also squeeze the batch dimension when b=1.\n",
    "        X.squeeze_(3).squeeze_(2)\n",
    "        X = self.linear(X)\n",
    "        # X = torch.sigmoid(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48714193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# pretrained_vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aba1f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 64\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "dropout=0.1\n",
    "\n",
    "transformer = Transformer(d_model=EMB_SIZE,\n",
    "           nhead=NHEAD,\n",
    "           num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "           num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "           dim_feedforward=512,\n",
    "           dropout=dropout)\n",
    "positional_encoding = PositionalEncoding(EMB_SIZE, dropout=dropout)\n",
    "\n",
    "enc = Encoder(EMB_SIZE, TGT_VOCAB_SIZE, transformer, positional_encoding).to(device)\n",
    "dec = Decoder(EMB_SIZE, TGT_VOCAB_SIZE, transformer, positional_encoding).to(device)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "embedder = Embedder().to(device)\n",
    "# embedder = Embedder_feat().to(device)\n",
    "extractor = Extractor().to(device)\n",
    "# extractor = ExtractorWithNoMixer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57d37831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(dec, [(16, 8, 64), (1, 8), (1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e6e217a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enc.load_state_dict(torch.load('./pretraining/encoder_6_depth_bleu_89.pth'))\n",
    "# dec.load_state_dict(torch.load('./pretraining/decoder_6_depth_bleu_89.pth'))\n",
    "enc.load_state_dict(torch.load('./encoder_64_50_noise.pth'))\n",
    "dec.load_state_dict(torch.load('./decoder_64_50_noise.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chkt_pth = './logs_end_to_end/vit_17.pth'\n",
    "# checkpoint = torch.load(chkt_pth)\n",
    "# embedder.load_state_dict(checkpoint['embedder_state_dict'])\n",
    "# extractor.load_state_dict(checkpoint['extractor_state_dict'])\n",
    "# discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "# # enc.load_state_dict(torch.load('./pretraining/encoder_6_depth_bleu_89.pth'))\n",
    "# # dec.load_state_dict(torch.load('./pretraining/decoder_6_depth_bleu_89.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8afc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle a way to load pre-trained ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf24a8",
   "metadata": {},
   "source": [
    "Best config is: {'lr': 1e-06, 'enc_lr': 1e-05, \n",
    "                 'dec_lr': 0.0001, 'wt_decay': 0.001, 'img_wt': 0.01,\n",
    "                 'emb_wt': 1.0, 'text_wt': 0.01, 'adv_wt': 0.0001}\n",
    "                 \n",
    "- {'lr': 0.0001, 'enc_lr': 1e-07, 'dec_lr': 1e-06, 'wt_decay': 1e-07, 'img_wt': 1.5, 'emb_wt': 1e-05, 'text_wt': 1e-05, 'adv_wt': 0.0001}\n",
    "\n",
    "- Best config is: {'lr': 1e-06, 'enc_lr': 1e-09, \n",
    "                 'dec_lr': 1e-07, 'wt_decay': 1e-07, 'img_wt': 0.5, \n",
    "                 'emb_wt': 1e-05, 'text_wt': 1e-05, 'adv_wt': 1e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb6245a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4#1e-8#1e-7#1e-4\n",
    "enc_lr = 1e-6#1e-7# 1e-8\n",
    "dec_lr = 1e-5#1e-6\n",
    "wt_decay = 1e-6\n",
    "params = [\n",
    "#     {'params':enc.parameters(),'lr':enc_lr},\n",
    "#     {'params':decoder.generator.parameters()},\n",
    "#     {'params':decoder.tgt_tok_emb.parameters()},\n",
    "    {'params':embedder.parameters()},\n",
    "    {'params':extractor.parameters()}\n",
    "]\n",
    "params_discrim = [{'params':discriminator.parameters()}]\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=1)\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "bce_with_logits_loss = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "ssim = SSIM().to(device)\n",
    "optimizer = optim.Adam(params, lr=learning_rate, weight_decay= wt_decay)\n",
    "optimizer_discrim = optim.Adam(params_discrim, lr=1e-8)\n",
    "lr_scheduling = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "055ef439",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol=BOS_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fabc74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rot_mat(theta):\n",
    "    theta = torch.tensor(theta)\n",
    "    return torch.tensor([[torch.cos(theta), -torch.sin(theta), 0],\n",
    "                         [torch.sin(theta), torch.cos(theta), 0]])\n",
    "\n",
    "\n",
    "def rotate_image_(x, angle):\n",
    "    dtype =  torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "    \n",
    "    rot_mat = get_rot_mat(angle)[None, ...].type(dtype).repeat(x.shape[0],1,1)\n",
    "    grid = F.affine_grid(rot_mat, x.size()).type(dtype)\n",
    "    x = F.grid_sample(x, grid)\n",
    "    return x\n",
    "\n",
    "def gaussian_blur(img, sigma_value):\n",
    "    b = T.GaussianBlur(kernel_size=(5, 9), sigma=sigma_value)\n",
    "    return b(img)\n",
    "\n",
    "def salt_paper(img, amount):\n",
    "    salt_img = torch.tensor(random_noise(img.detach().cpu().numpy(), mode='salt', amount=amount)).to(device)\n",
    "    return salt_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3eee8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol=BOS_IDX\n",
    "def train(dataloader, teacher_forcing_ratio=0.5):\n",
    "    '''teacher forcing plus greedy decoding method'''\n",
    "    embedder.train()\n",
    "    extractor.train()\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "#     enc.train()\n",
    "#     dec.train()\n",
    "    discriminator.train()\n",
    "    cover_label = 1\n",
    "    encoded_label = 0\n",
    "    train_loss = []\n",
    "    \n",
    "    embedding_loss = []\n",
    "    image_loss = []\n",
    "    text_loss = []\n",
    "    ssim_total = []\n",
    "    adversarial_bce = []\n",
    "    discr_cover_bce = []\n",
    "    discr_encod_bce = []\n",
    "    trg_vocab_size = dec.vocab_size\n",
    "    \n",
    "    for data in dataloader:\n",
    "        img = data[0].to(device)\n",
    "        sen = data[1].to(device)\n",
    "#         sen = torch.transpose(sen, -1, 0)\n",
    "        \n",
    "        batch_size = sen.shape[1]\n",
    "        max_len = sen.shape[0]\n",
    "        trg_vocab_size = dec.vocab_size\n",
    "        tgt_input = sen[:-1, :]\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(sen, sen)\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
    "    \n",
    "        with torch.enable_grad():\n",
    "            optimizer_discrim.zero_grad()\n",
    "            # train discriminator\n",
    "            d_target_label_cover = torch.full((batch_size, 1), cover_label, device=device, dtype=torch.float32)\n",
    "            d_target_label_encoded = torch.full((batch_size, 1), encoded_label, device=device, dtype=torch.float32)\n",
    "            g_target_label_encoded = torch.full((batch_size, 1), cover_label, device=device, dtype=torch.float32)\n",
    "            \n",
    "            d_on_cover = discriminator(img)\n",
    "            d_loss_on_cover = bce_with_logits_loss(d_on_cover, d_target_label_cover)\n",
    "            d_loss_on_cover.backward()\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "            encoder_outputs = enc(sen, src_mask, src_padding_mask)\n",
    "#                 inputs = feature_extractor(img, return_tensors=\"pt\")\n",
    "#                 outputs_ = pretrained_vit(**inputs)\n",
    "\n",
    "#             last_hidden_states = outputs_.last_hidden_state\n",
    "#             img_features = last_hidden_states[:, :-1, :].to(device)\n",
    "            watermark_img = embedder(img, encoder_outputs)\n",
    "            # train on fake\n",
    "            d_on_encoded = discriminator(watermark_img.detach())\n",
    "            d_loss_on_encoded = bce_with_logits_loss(d_on_encoded, d_target_label_encoded)\n",
    "\n",
    "            d_loss_on_encoded.backward()\n",
    "            optimizer_discrim.step()\n",
    "            \n",
    "            ssim_img = ssim(watermark_img, img)\n",
    "    \n",
    "            d_on_encoded_for_enc = discriminator(watermark_img)\n",
    "            g_loss_adv = bce_with_logits_loss(d_on_encoded_for_enc, g_target_label_encoded)\n",
    "            watermark_text = extractor(watermark_img)\n",
    "            \n",
    "\n",
    "            ys = torch.ones(1, batch_size).fill_(start_symbol).type(torch.long).to(device)\n",
    "#             pdb.set_trace()\n",
    "            for i in range(max_len-1):\n",
    "                memory = watermark_text.to(device)\n",
    "                tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                            .type(torch.bool)).to(device)\n",
    "#                 with torch.no_grad():\n",
    "                out = dec.decode(ys, memory, tgt_mask)\n",
    "                out = out.transpose(0, 1)\n",
    "                prob = dec.generator(out[:, -1])\n",
    "                outputs[i] = prob\n",
    "                _, next_word = torch.max(prob, dim=1)\n",
    "                \n",
    "                teacher_force = random.random() < 0.5\n",
    "                if teacher_force:\n",
    "                    next_word = sen[i]\n",
    "                ys = torch.cat([ys, next_word.unsqueeze(dim=0)], dim=0)\n",
    "               \n",
    "            loss_img = mse_loss(img, watermark_img)\n",
    "\n",
    "            loss_emb = mse_loss(encoder_outputs, watermark_text)\n",
    "\n",
    "            text_out = outputs[1:].view(-1, outputs.shape[-1])\n",
    "    #         text = sen.type(torch.long)\n",
    "            sen = sen[1:].view(-1)\n",
    "    #         print(text.device, sen.device)\n",
    "    #         pdb.set_trace()\n",
    "            loss_text = loss_fn(text_out, sen)\n",
    "\n",
    "#             loss = 0.1*loss_img + 0.8*loss_emb + 0.01*loss_text + 0.0001*g_loss_adv\n",
    "#             loss = 0.01*loss_img + 1.0*loss_emb + 0.01*loss_text + 0.0001*g_loss_adv\n",
    "            loss = 3*loss_img + 0.5*loss_emb + 0.8*loss_text + 0.0001*g_loss_adv - 0.01*ssim_img\n",
    "#             loss = 0.5*loss_img + 1e-05*loss_emb + 1e-05*loss_text + 1e-05*g_loss_adv\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             lr_scheduling.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        embedding_loss.append(loss_emb.item())\n",
    "        image_loss.append(loss_img.item())\n",
    "        text_loss.append(loss_text.item())\n",
    "        ssim_total.append(ssim_img.item())\n",
    "        adversarial_bce.append(g_loss_adv.item())\n",
    "        discr_cover_bce.append(d_loss_on_cover.item())\n",
    "        discr_encod_bce.append(d_loss_on_encoded.item())\n",
    "  \n",
    "    return np.mean(train_loss),np.mean(embedding_loss), np.mean(image_loss), np.mean(text_loss), np.mean(ssim_total), np.mean(adversarial_bce), np.mean(discr_cover_bce), np.mean(discr_encod_bce)\n",
    "\n",
    "\n",
    "def valid(dataloader, teacher_forcing_ratio=0.0):\n",
    "    embedder.eval()\n",
    "    extractor.eval()\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    train_loss = []\n",
    "    cover_label = 1\n",
    "    encoded_label = 0\n",
    "    \n",
    "    embedding_loss = []\n",
    "    image_loss = []\n",
    "    text_loss = []\n",
    "    ssim_total = []\n",
    "    adversarial_bce = []\n",
    "    discr_cover_bce = []\n",
    "    discr_encod_bce = []\n",
    "    trg_vocab_size = dec.vocab_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            img = data[0].to(device)\n",
    "            sen = data[1].to(device)\n",
    "            batch_size = sen.shape[1]\n",
    "            max_len = sen.shape[0]\n",
    "            tgt_input = sen[:-1, :]\n",
    "        \n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(sen, sen)\n",
    "            d_target_label_cover = torch.full((batch_size, 1), cover_label, device=device, dtype=torch.float32)\n",
    "            d_target_label_encoded = torch.full((batch_size, 1), encoded_label, device=device, dtype=torch.float32)\n",
    "            g_target_label_encoded = torch.full((batch_size, 1), cover_label, device=device, dtype=torch.float32)\n",
    "\n",
    "            outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
    "            \n",
    "            # train on real image\n",
    "            d_on_cover = discriminator(img)\n",
    "            d_loss_on_cover = bce_with_logits_loss(d_on_cover, d_target_label_cover)\n",
    "            \n",
    "            encoder_outputs = enc(sen, src_mask, src_padding_mask)\n",
    "#             inputs = feature_extractor(img, return_tensors=\"pt\")\n",
    "#             outputs_ = pretrained_vit(**inputs)\n",
    "\n",
    "#             last_hidden_states = outputs_.last_hidden_state\n",
    "#             img_features = last_hidden_states[:, :-1, :].to(device)\n",
    "            watermark_img = embedder(img, encoder_outputs)\n",
    "            # train on fake\n",
    "            d_on_encoded = discriminator(watermark_img)\n",
    "            d_loss_on_encoded = bce_with_logits_loss(d_on_encoded, d_target_label_encoded)\n",
    "            \n",
    "            ssim_img = ssim(watermark_img, img)\n",
    "            # 1. rotation\n",
    "#             rotation_angle = random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "#             watermark_img = rotate_image_(watermark_img, (np.pi/180)*rotation_angle)\n",
    "            \n",
    "            # 2. gaussian blurr\n",
    "#             watermark_img = gaussian_blur(watermark_img)\n",
    "\n",
    "            # 3. random additive gaussian noise\n",
    "#             mean = random.randint(0,2)\n",
    "#             std = random.randint(0,2)\n",
    "#             watermark_img = watermark_img + normal_noise(watermark_img.shape, mean, std).to(device)\n",
    "            \n",
    "            # on embedder-extractor\n",
    "            d_on_encoded_for_enc = discriminator(watermark_img)\n",
    "            g_loss_adv = bce_with_logits_loss(d_on_encoded_for_enc, g_target_label_encoded)\n",
    "            watermark_text = extractor(watermark_img)\n",
    "            \n",
    "            # embedding noise\n",
    "#             watermark_text = watermark_text + normal_noise(watermark_text.shape).to(device)\n",
    "            \n",
    "            ys = torch.ones(1, batch_size).fill_(start_symbol).type(torch.long).to(device)\n",
    "#             pdb.set_trace()\n",
    "            for i in range(max_len-1):\n",
    "                memory = watermark_text.to(device)\n",
    "                tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                            .type(torch.bool)).to(device)\n",
    "\n",
    "                out = dec.decode(ys, memory, tgt_mask)\n",
    "                out = out.transpose(0, 1)\n",
    "                prob = dec.generator(out[:, -1])\n",
    "                outputs[i] = prob\n",
    "                _, next_word = torch.max(prob, dim=1)\n",
    "                \n",
    "                teacher_force = random.random() < 0.5\n",
    "                if teacher_force:\n",
    "                    next_word = sen[i]\n",
    "                ys = torch.cat([ys, next_word.unsqueeze(dim=0)], dim=0)\n",
    "                \n",
    "            loss_img = mse_loss(img, watermark_img)\n",
    "            loss_emb = mse_loss(encoder_outputs, watermark_text)\n",
    "\n",
    "            text_out = outputs[1:].view(-1, outputs.shape[-1])\n",
    "#             text = sen.type(torch.long)\n",
    "            sen = sen[1:].view(-1)\n",
    "\n",
    "            loss_text = loss_fn(text_out, sen)\n",
    "\n",
    "#             loss = 0.1*loss_img + 0.8*loss_emb + 0.1*loss_text\n",
    "#             loss = 0.01*loss_img + 1.0*loss_emb + 0.01*loss_text + 0.0001*g_loss_adv\n",
    "#             loss = 0.1*loss_img + 0.8*loss_emb + 0.01*loss_text + 0.0001*g_loss_adv\n",
    "#             loss = 0.1*loss_img + 0.1*loss_emb + 1*loss_text #+ 0.001*g_loss_adv\n",
    "#             loss = 0.5*loss_img + 1e-05*loss_emb + 1*loss_text + 0.0001*g_loss_adv\n",
    "            loss = 3*loss_img + 0.5*loss_emb + 0.8*loss_text + 0.0001*g_loss_adv - 0.01*ssim_img\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "            embedding_loss.append(loss_emb.item())\n",
    "            image_loss.append(loss_img.item())\n",
    "            text_loss.append(loss_text.item())\n",
    "            ssim_total.append(ssim_img.item())\n",
    "#             adversarial_bce.append(g_loss_adv.item())\n",
    "#             discr_cover_bce.append(d_loss_on_cover.item())\n",
    "#             discr_encod_bce.append(d_loss_on_encoded.item())\n",
    "  \n",
    "    return np.mean(train_loss),np.mean(embedding_loss), np.mean(image_loss), np.mean(text_loss), np.mean(ssim_total), np.mean(adversarial_bce), np.mean(discr_cover_bce), np.mean(discr_encod_bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21a0a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model_state, filename):\n",
    "    torch.save(model_state, filename) \n",
    "\n",
    "chkt_pth = './logs_end_to_end/vit_19_50_noise.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fed4a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched src_key_padding_mask and src_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  5.841745628145006\n",
      "Val loss:  5.222255863189697\n",
      "Embedding loss:  0.764225413163503\n",
      "Image loss:  0.03860407503694296\n",
      "Text loss:  6.690085576799181\n",
      "Val Text loss:  6.021884838104248\n",
      "Ssim:  0.8319818954070409\n",
      "Epoch:  1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/data/anaconda3/envs/bishwa_l3/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  4.262830758836534\n",
      "Val loss:  3.7788007640838623\n",
      "Embedding loss:  0.6072110689481099\n",
      "Image loss:  0.039611137222912575\n",
      "Text loss:  4.810531454934014\n",
      "Val Text loss:  4.200661664962769\n",
      "Ssim:  0.810527108722263\n",
      "Epoch:  2/250\n",
      "Train loss:  2.746048644595676\n",
      "Val loss:  2.4328186559677123\n",
      "Embedding loss:  0.5093785333633423\n",
      "Image loss:  0.061222659789853626\n",
      "Text loss:  2.8941591180165607\n",
      "Val Text loss:  2.4967127532958986\n",
      "Ssim:  0.7707700503137377\n",
      "Epoch:  3/250\n",
      "Train loss:  1.77737084335751\n",
      "Val loss:  1.824173373222351\n",
      "Embedding loss:  0.4911063054667579\n",
      "Image loss:  0.058435042354795665\n",
      "Text loss:  1.7054780943658616\n",
      "Val Text loss:  1.7795986118316651\n",
      "Ssim:  0.7941702903641594\n",
      "Epoch:  4/250\n",
      "Train loss:  1.4314528321160211\n",
      "Val loss:  1.704262755393982\n",
      "Embedding loss:  0.4751782041920556\n",
      "Image loss:  0.049302348696523245\n",
      "Text loss:  1.3176827246877882\n",
      "Val Text loss:  1.6603459177017212\n",
      "Ssim:  0.8261287874115838\n",
      "Epoch:  5/250\n",
      "Train loss:  1.291677886221144\n",
      "Val loss:  1.5599921073913574\n",
      "Embedding loss:  0.44898456494013467\n",
      "Image loss:  0.0432793958319558\n",
      "Text loss:  1.182172544002533\n",
      "Val Text loss:  1.5210952801704407\n",
      "Ssim:  0.8462416785558065\n",
      "Epoch:  6/250\n",
      "Train loss:  1.2279419264263578\n",
      "Val loss:  1.5071891984939576\n",
      "Embedding loss:  0.4304683101177216\n",
      "Image loss:  0.0396848386178414\n",
      "Text loss:  1.1276985912852817\n",
      "Val Text loss:  1.4827812113761902\n",
      "Ssim:  0.8577410265604655\n",
      "Epoch:  7/250\n",
      "Train loss:  1.1875326869222853\n",
      "Val loss:  1.4467846088409424\n",
      "Embedding loss:  0.41651654360029433\n",
      "Image loss:  0.0375430243478881\n",
      "Text loss:  1.0940320608880785\n",
      "Val Text loss:  1.420377025604248\n",
      "Ssim:  0.8652091002464295\n",
      "Epoch:  8/250\n",
      "Train loss:  1.1500486738416884\n",
      "Val loss:  1.3895953092575073\n",
      "Embedding loss:  0.39828313247362773\n",
      "Image loss:  0.035204366793235145\n",
      "Text loss:  1.0674412160449558\n",
      "Val Text loss:  1.3747946019172668\n",
      "Ssim:  0.8730743177466922\n",
      "Epoch:  9/250\n",
      "Train loss:  1.1315562846925524\n",
      "Val loss:  1.4610102968215943\n",
      "Embedding loss:  0.389891248650021\n",
      "Image loss:  0.034419319301843644\n",
      "Text loss:  1.052553343296051\n",
      "Val Text loss:  1.459517032146454\n",
      "Ssim:  0.8761742536226909\n",
      "Epoch:  10/250\n",
      "Train loss:  1.1043174435827467\n",
      "Val loss:  1.3757664680480957\n",
      "Embedding loss:  0.3778147818512387\n",
      "Image loss:  0.032091563827461665\n",
      "Text loss:  1.0348665414386324\n",
      "Val Text loss:  1.3681760892868042\n",
      "Ssim:  0.882963099108802\n",
      "Epoch:  11/250\n",
      "Train loss:  1.0822455729378595\n",
      "Val loss:  1.3671509103775024\n",
      "Embedding loss:  0.3682828914059533\n",
      "Image loss:  0.030266667895846897\n",
      "Text loss:  1.0201164385477701\n",
      "Val Text loss:  1.3694210605621338\n",
      "Ssim:  0.8860774549908108\n",
      "Epoch:  12/250\n",
      "Train loss:  1.073103628423479\n",
      "Val loss:  1.3428180832862855\n",
      "Embedding loss:  0.36448528626230026\n",
      "Image loss:  0.029522617037097614\n",
      "Text loss:  1.0138953484429254\n",
      "Val Text loss:  1.358332452774048\n",
      "Ssim:  0.8894880485004849\n",
      "Epoch:  13/250\n",
      "Train loss:  1.0524864701694912\n",
      "Val loss:  1.3110608010292053\n",
      "Embedding loss:  0.35657887562115986\n",
      "Image loss:  0.027570731106731628\n",
      "Text loss:  1.0004516201019287\n",
      "Val Text loss:  1.32694398021698\n",
      "Ssim:  0.8948173454602559\n",
      "Epoch:  14/250\n",
      "Train loss:  1.0451232864061992\n",
      "Val loss:  1.3225776553153992\n",
      "Embedding loss:  0.35228675458166336\n",
      "Image loss:  0.026751308793822925\n",
      "Text loss:  0.9970259243117439\n",
      "Val Text loss:  1.3555813093185425\n",
      "Ssim:  0.8966450799836053\n",
      "Epoch:  15/250\n",
      "Train loss:  1.0365006723403931\n",
      "Val loss:  1.3320898838043214\n",
      "Embedding loss:  0.348039375172721\n",
      "Image loss:  0.026047021826108296\n",
      "Text loss:  0.9915791150199043\n",
      "Val Text loss:  1.3609533200263977\n",
      "Ssim:  0.8995047877629598\n",
      "Epoch:  16/250\n",
      "Train loss:  1.0287379885779486\n",
      "Val loss:  1.3339113278388977\n",
      "Embedding loss:  0.34475769551595054\n",
      "Image loss:  0.025181191843416956\n",
      "Text loss:  0.9872010970645481\n",
      "Val Text loss:  1.3466788439750672\n",
      "Ssim:  0.9016967666414049\n",
      "Epoch:  17/250\n",
      "Train loss:  1.0224131530655756\n",
      "Val loss:  1.2960761060714723\n",
      "Embedding loss:  0.3413782098558214\n",
      "Image loss:  0.02419640968574418\n",
      "Text loss:  0.9851184299786886\n",
      "Val Text loss:  1.3360290446281433\n",
      "Ssim:  0.9031555809974671\n",
      "Epoch:  18/250\n",
      "Train loss:  1.0059894024001228\n",
      "Val loss:  1.3137253365516663\n",
      "Embedding loss:  0.33387805783748625\n",
      "Image loss:  0.022629553503460353\n",
      "Text loss:  0.9752054594887628\n",
      "Val Text loss:  1.3363485412597655\n",
      "Ssim:  0.9074248030450609\n",
      "Epoch:  19/250\n",
      "Train loss:  1.0088591462771097\n",
      "Val loss:  1.2651529202461242\n",
      "Embedding loss:  0.33546959522035386\n",
      "Image loss:  0.022890414946609074\n",
      "Text loss:  0.976825334125095\n",
      "Val Text loss:  1.2972886462211608\n",
      "Ssim:  0.9078729138374328\n",
      "Epoch:  20/250\n",
      "Train loss:  0.9875945582919651\n",
      "Val loss:  1.2417092742919922\n",
      "Embedding loss:  0.3249131221638785\n",
      "Image loss:  0.020460804326666727\n",
      "Text loss:  0.9660189491377936\n",
      "Val Text loss:  1.2970403275489808\n",
      "Ssim:  0.9131091837353177\n",
      "Epoch:  21/250\n",
      "Train loss:  0.9824460979037815\n",
      "Val loss:  1.279930950641632\n",
      "Embedding loss:  0.32026625862386493\n",
      "Image loss:  0.020184058412081665\n",
      "Text loss:  0.9635430884361267\n",
      "Val Text loss:  1.277898633480072\n",
      "Ssim:  0.9145156318876478\n",
      "Epoch:  22/250\n",
      "Train loss:  0.9710868168407016\n",
      "Val loss:  1.2778392024040222\n",
      "Embedding loss:  0.31459774973657395\n",
      "Image loss:  0.019162211525771352\n",
      "Text loss:  0.9567454547352261\n",
      "Val Text loss:  1.3226579785346986\n",
      "Ssim:  0.9166486584875319\n",
      "Epoch:  23/250\n",
      "Train loss:  0.9645015595753987\n",
      "Val loss:  1.245120020389557\n",
      "Embedding loss:  0.3086364919344584\n",
      "Image loss:  0.01881052749686771\n",
      "Text loss:  0.9535733311971029\n",
      "Val Text loss:  1.2907749228477479\n",
      "Ssim:  0.9178313375578986\n",
      "Epoch:  24/250\n",
      "Train loss:  0.9514283275074429\n",
      "Val loss:  1.2342635116577148\n",
      "Embedding loss:  0.2994314062330458\n",
      "Image loss:  0.0175851126263539\n",
      "Text loss:  0.9476305102772183\n",
      "Val Text loss:  1.2885250067710876\n",
      "Ssim:  0.9218431321779886\n",
      "Epoch:  25/250\n",
      "Train loss:  0.9335191778606838\n",
      "Val loss:  1.2079432244300843\n",
      "Embedding loss:  0.29124024119642045\n",
      "Image loss:  0.015705243660344016\n",
      "Text loss:  0.9374711023966471\n",
      "Val Text loss:  1.2756698718070985\n",
      "Ssim:  0.9264785845014784\n",
      "Epoch:  26/250\n"
     ]
    }
   ],
   "source": [
    "history = {'train_loss': [], 'val_loss': [], 'train_emb_loss': [], 'train_img_loss':[], 'train_text_loss':[],\n",
    "           'val_emb_loss': [], 'val_img_loss':[], 'val_text_loss':[], 'train_ssim':[], 'val_ssim':[]}\n",
    "\n",
    "num_epochs=250\n",
    "# encoder=None\n",
    "# decoder=None\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \", str(epoch)+'/'+str(num_epochs))\n",
    "    train_loss, emb_loss, img_loss, text_loss, train_ssim, _, _, _ = train(zip(train_dataloader_img, train_iter_w))\n",
    "    val_loss, emb_loss_val, img_loss_val, text_loss_val, val_ssim, _, _, _ = valid(zip(val_dataloader_img, valid_iter_w))\n",
    "    \n",
    "    model_state = {\n",
    "        'epoch': epoch,\n",
    "        'encoder_state_dict': enc.state_dict(),\n",
    "        'decoder_state_dict': dec.state_dict(),\n",
    "        'embedder_state_dict': embedder.state_dict(),\n",
    "        'extractor_state_dict': extractor.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'dis_optim_state_dict': optimizer_discrim.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'emb_loss': emb_loss,\n",
    "        'img_loss': img_loss,\n",
    "        'ssim': train_ssim,\n",
    "        'text_loss': text_loss\n",
    "    }\n",
    "#     save_checkpoint(model_state, chkt_pth)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    print(\"Train loss: \", train_loss)\n",
    "    print(\"Val loss: \", val_loss)\n",
    "    history['train_emb_loss'].append(emb_loss)\n",
    "    history['train_img_loss'].append(img_loss)\n",
    "    history['train_text_loss'].append(text_loss)\n",
    "    history['train_ssim'].append(train_ssim)\n",
    "    \n",
    "    history['val_emb_loss'].append(emb_loss_val)\n",
    "    history['val_img_loss'].append(img_loss_val)\n",
    "    history['val_text_loss'].append(text_loss_val)\n",
    "    history['val_ssim'].append(val_ssim)\n",
    "\n",
    "    print(\"Embedding loss: \", emb_loss)\n",
    "    print(\"Image loss: \", img_loss)\n",
    "    print(\"Text loss: \", text_loss)\n",
    "    print(\"Val Text loss: \", text_loss_val)\n",
    "    print(\"Ssim: \", train_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19bc5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['train_loss'])+1)\n",
    "plt.plot(epochs, history['train_emb_loss'], label='Train Embedding loss')\n",
    "plt.plot(epochs, history['val_emb_loss'], label='Val Embedding loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['train_loss'])+1)\n",
    "plt.plot(epochs, history['train_img_loss'], label='Train Image loss')\n",
    "plt.plot(epochs, history['val_img_loss'], label='Val Image loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['train_loss'])+1)\n",
    "plt.plot(epochs, history['train_text_loss'], label='Train Text loss')\n",
    "plt.plot(epochs, history['val_text_loss'], label='Val Text loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history['train_loss'])+1)\n",
    "plt.plot(epochs, history['train_ssim'], label='Train SSIM')\n",
    "plt.plot(epochs, history['val_ssim'], label='Val SSIM')\n",
    "plt.title('SSIM')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5a1a7",
   "metadata": {},
   "source": [
    "- pre-train ViT for image classification task and use few layers in here, this might need more data to train\n",
    "- or use pre-trained ViT's weight? -> this might decrease our current number of trainable parameters\n",
    "   - but again for this we might need to use others implementation\n",
    "- merge Transformer based text encoder-decoder\n",
    "\n",
    "- train this current model for larger epochs and see the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6700c5c7",
   "metadata": {},
   "source": [
    "## Decoding process in Transformer?\n",
    "\n",
    "- Autoregressive or not?\n",
    "- Current issue: I passed all previous outputs by concatenating but decoder produces output for all the given input, meaning it again produces output for the previous output\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/70146811/feed-decoder-input-in-transformers?rq=3\n",
    "- How does decoder run multiple times ? confusion or How does the iteration happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6f7bd",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdb0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkt_pth = './logs_end_to_end/vit_16.pth'\n",
    "checkpoint = torch.load(chkt_pth)\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint['ssim'], checkpoint['epoch'], checkpoint['text_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db00d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedder, 'embedder.pth',)\n",
    "torch.save(extractor, 'extractor.pth')\n",
    "torch.save(discriminator, 'discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.load_state_dict(checkpoint['embedder_state_dict'])\n",
    "extractor.load_state_dict(checkpoint['extractor_state_dict'])\n",
    "enc.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "dec.load_state_dict(checkpoint['decoder_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b4a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, samples=1):\n",
    "    embedder.eval()\n",
    "    extractor.eval()\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    trg_vocab_size = dec.vocab_size\n",
    "    data = next(iter(dataset))\n",
    "    print(data[0].shape, data[1].shape)\n",
    "    img = data[0].to(device)\n",
    "    sen = data[1].to(device)\n",
    "#     sen = sen.permute(1, 0)\n",
    "    \n",
    "    batch_size = sen.shape[1]\n",
    "    max_len = sen.shape[0]\n",
    "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(sen, sen)\n",
    "\n",
    "#     outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
    "    outputs = torch.zeros(max_len, batch_size).to(device)\n",
    "    \n",
    "    encoder_outputs = enc(sen, src_mask, src_padding_mask)\n",
    "    watermark_img_no_noise = embedder(img, encoder_outputs)\n",
    "    watermark_img = watermark_img_no_noise\n",
    "\n",
    "    # 1. rotation\n",
    "#     watermark_img = rotate_image_(watermark_img, (np.pi/180)*5)\n",
    "\n",
    "    # 2. gaussian blurr\n",
    "#     watermark_img = gaussian_blur(watermark_img, 1)\n",
    "\n",
    "    # 3. crop\n",
    "#     watermark_img = center_crop(watermark_img, 20)\n",
    "\n",
    "#     5. salt paper\n",
    "#     watermark_img = salt_paper(watermark_img, 0.1)\n",
    "\n",
    "    # 6. cropout\n",
    "#         watermark_img = cropout(watermark_img, [0.1, 0.4], [0.1, 0.4])\n",
    "\n",
    "    watermark_text = extractor(watermark_img)\n",
    "    loss_emb = mse_loss(encoder_outputs, watermark_text)\n",
    "    # embedding noise\n",
    "#         watermark_text = watermark_text + normal_noise(watermark_text.shape).to(device)\n",
    "#         watermark_text = watermark_text + random_noise(watermark_text, -0.5, 0.5)\n",
    "\n",
    "    ys = torch.ones(1, batch_size).fill_(start_symbol).type(torch.long).to(device)\n",
    "    for i in range(max_len-1):\n",
    "        memory = watermark_text.to(device)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(device)\n",
    "#             pdb.set_trace()\n",
    "        out = dec.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = dec.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        outputs[i] = next_word\n",
    "\n",
    "        ys = torch.cat([ys, next_word.unsqueeze(dim=0)], dim=0)\n",
    "\n",
    "    ssim_img = ssim(watermark_img_no_noise, img).detach().cpu().numpy()\n",
    "#     ssim_img = 0\n",
    "    watermark = watermark_img.detach().cpu().numpy()\n",
    "    img = img.detach().cpu().numpy()\n",
    "    watermark = np.moveaxis(watermark, 1, -1)\n",
    "    img = np.moveaxis(img, 1, -1)\n",
    "\n",
    "    watermark_img_no_noise = watermark_img_no_noise.detach().cpu().numpy()\n",
    "    watermark_img_no_noise = np.moveaxis(watermark_img_no_noise, 1, -1)\n",
    "    \n",
    "#     pdb.set_trace()\n",
    "    outputs = outputs.type(torch.int)\n",
    "    outputs = torch.transpose(outputs, -1, 0)\n",
    "    outputs = outputs.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    inputs = torch.transpose(sen, -1, 0)\n",
    "    inputs = inputs.detach().cpu().numpy().tolist()\n",
    "    #     batch_size = sentence.shape[1]\n",
    "#     pdb.set_trace()\n",
    "    translated_sentence = []\n",
    "    input_sentence = []\n",
    "    for i in range(batch_size):\n",
    "        token_ = outputs[i]\n",
    "        in_ = inputs[i]\n",
    "        sen = vocab_transform[TGT_LANGUAGE].lookup_tokens(token_)\n",
    "        in_sen = vocab_transform[TGT_LANGUAGE].lookup_tokens(in_)\n",
    "        translated_sentence.append(' '.join(x for x in sen[1:]))\n",
    "        input_sentence.append(' '.join(x for x in in_sen[1:]))\n",
    "\n",
    "    return img, watermark, watermark_img_no_noise, input_sentence, translated_sentence, np.mean(ssim_img), encoder_outputs, watermark_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1451aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, watermark, watermark_img_no_noise, input_sentence, output_sentence, ssim_ = predict(zip(train_dataloader_img, train_iter_w))\n",
    "img, watermark, watermark_img_no_noise, input_sentence, output_sentence, ssim_, e_o, w_t = predict(zip(val_dataloader_img, valid_iter_w))\n",
    "# img, watermark, watermark_img_no_noise, input_sentence, output_sentence, ssim = predict(zip(test_dataloader_img, test_iter_w))\n",
    "for i in range(len(input_sentence)):\n",
    "    print(\"Input: \", input_sentence[i].split('<eos>')[0].split('<pad>')[0])\n",
    "#     print(input_sentence[i])\n",
    "    print(\"Output: \", output_sentence[i].split('<eos>')[0])\n",
    "    print(\"**\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f072d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(input_sentence)):\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "        fig.add_subplot(241)\n",
    "        plt.title('cover image')\n",
    "        plt.axis('off')\n",
    "        l = len(input_sentence[i].split('<eos>')[0].split('<pad>')[0])\n",
    "        print(\"Input sentence: \", input_sentence[i].split('<eos>')[0].split('<pad>')[0])\n",
    "        plt.imshow(img[i])\n",
    "        \n",
    "#         fig.add_subplot(242)\n",
    "#         plt.title('No noise watermarked image')\n",
    "#         plt.axis('off')\n",
    "#         plt.imshow(watermark_img_no_noise[i])\n",
    "        \n",
    "        fig.add_subplot(242)\n",
    "        plt.title('watermark image')\n",
    "        plt.axis('off')\n",
    "        print('**'*10)\n",
    "        print(\"Output sentence: \", output_sentence[i].split('<eos>')[0][:l])\n",
    "        plt.imshow(watermark[i])\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_blue_score(input_sentence, predicted_sentence):\n",
    "    input_token = []\n",
    "    predicted_token = []\n",
    "    unk = ['<unk>']\n",
    "    for in_, pre_ in zip(input_sentence, predicted_sentence):\n",
    "        input_sen = in_.split('<eos>')[0].split('<pad>')[0]\n",
    "#         l = len(input_sen)\n",
    "#         input_sen = in_.strip().split('<eos>')[0].split('<pad>')[0]\n",
    "        input_sen = input_sen.split(' ')\n",
    "        \n",
    "        n = [i for i in range(len(input_sen)) if input_sen[i] in unk]\n",
    "        input_sen = ' '.join(word for word in input_sen if word not in unk).strip()\n",
    "        l = len(input_sen.split(' '))\n",
    "        \n",
    "        predicted_sen = pre_.split('<eos>')[0].split(' ')[:l]\n",
    "#         predicted_sen = predicted_sen.strip().split(' ')\n",
    "        c = 0\n",
    "        for i in n:\n",
    "            try:\n",
    "                del predicted_sen[i-c]\n",
    "                c += 1\n",
    "            except:\n",
    "                del predicted_sen[i-1]\n",
    "            \n",
    "        input_token.append([input_sen.split(' ')])\n",
    "        predicted_token.append(predicted_sen)\n",
    "        \n",
    "    return bleu_score(predicted_token, input_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa16fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_blue_score(input_sentence, output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8643447",
   "metadata": {},
   "source": [
    "# Analyzing encoder outputs and extractor output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[2, 3]]], dtype=torch.float32)\n",
    "y = torch.tensor([[[4, 5]]], dtype=torch.float32)\n",
    "torch.square(y - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac03211",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.square(y-x), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea152c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e_o.shape, w_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c320562",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.mean(torch.square(e_o - w_t), dim=2), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa563a",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- Analyze recent architecture\n",
    "- Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfc387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ai.googleblog.com/2020/04/optimizing-multiple-loss-functions-with.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff34216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
